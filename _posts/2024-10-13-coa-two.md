---
layout: post
title: 计算机组成原理学习笔记（二）
subtitle: Notes Pt.2 for review
tag: [Computer Organization and Architecture]
cover-img: /assets/img/miku.jpg
share-img: /assets/img/miku.jpg
thumbnail-img: /assets/img/miku.jpg
author: LiPtP
mathjax: true
---

{: .box-note}
This is a summary note of the course "Computer Organization and Architecture" by Professor *William Stallings*. 

The remaining contents is organized as:
- Chapter 4 (*Pt. II*)
    - Elements of Cache Design
        - Cache Addresses
        - Cache Size
        - Mapping Function
        - Replacement Algorithm
        - Write Policy
        - Line/Block Size
        - Number of Caches


-------

# Chapter 4 (Part II): Elements of Cache Design
## Cache Addresses
There are some concepts:
1. Virtual Memory: Virtual memory is a technique that allows programs to run without running out of physical memory. It works by mapping the virtual memory addresses to physical memory addresses.
2. Hardware Memory Management Unit (MMU): The MMU is a hardware device that translates virtual memory addresses to physical memory addresses. It is inserted **between Processor and Main Memory.**
3. Logical Cache is located between the MMU and **the processor**.
4. Physical Cache is located between the MMU and **the main memory**.

## Cache Size

{: .box-success}
*We would like the size of the cache to be small enough so that the overall average cost per bit is close to that of main memory alone and large enough so that the overall average access time is close to that of the cache alone.*

## Mapping Function
a means is needed for determining which **main memory block** currently occupies a cache
line. The choice of the mapping function dictates how the cache is organized.

So there are three main mapping functions:
1. Direct-mapped: Each block is mapped to a single cache line in main memory.
 
The corresponding way is: $$i = j mod m$$, where m is the number of cache lines and j is the main memory block number. It is very easy to implement, but it leads to fixed cache location for any given block. Thus, when two blocks are mapped to the same cache line, they will be evicted together. Here is an example:

<br/>
    ![Direct Mapping](/assets/img/COA-images/Direct-Mapping.png){: .mx-auto.d-block :}
    <br/>

2. Fully Associative Mapping

It overcomes the disadvantage of direct mapping by permitting each main memory block to be loaded into any line of the cache. In this way, the content in the main memory includes only **tag** and **word**.
3. (Fully) associative: Each cache line is mapped to any block in main memory.
# Review Questions
## Questions
1. What are the differences among sequential, direct, and random access methods?
2. What is the general relationship among assess time, memory cost and capacity?
3. How does the principle of locality relate to the use of multiple memory levels?

## Answers for reference

**Question 1:**

{: .box-note}
Sequential access is accessing data in a specific linear sequence (example: tapes).<br/>Direct access has the data address based on a physical location.<br/>With random access, any location can be selected at random, and the addressable locations in memory have a unique, physically wired-in addressing mechanism.

**Question 2:**

{: .box-note}
As access time becomes faster, the cost per bit increases. As memory size increases, the cost per bit is smaller. Also, with greater capacity, the access time becomes slower.

**Question 3:**

{: .box-note}
Slower and less expensive memory is used in higher stages, with the most expensive being the registers in the processor as well as cache. Main memory is slower, less expensive, and is outside of the processor.
