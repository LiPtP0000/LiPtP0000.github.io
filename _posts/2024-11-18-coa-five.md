---
layout: post
title: 计算机组成原理学习笔记（五）
subtitle: Processor Function and Organization
tag: [Computer Organization and Architecture]
cover-img: /assets/img/miku.jpg
share-img: /assets/img/miku.jpg
thumbnail-img: /assets/img/miku.jpg
author: LiPtP
mathjax: true
---

{: .box-note}
This is a summary note of the course "Computer Organization and Architecture" by Professor _William Stallings_.

The remaining contents are organized as:

- Chapter 14
    - Processor Organization
    - Register Organization
    - Instruction Cycle
    - Pipelining

# Chapter 14: Processor Structure and Function

## Processor Organization

First consider the *requirements* of the processor (or CPU, **C**entral **P**rocessing **U**nit).

- Fetch Instruction (FI)
- Interpret Instruction (DI)
- Fetch Data (FO)
- Process Data (CO, EI)
- Write Data (WO)

A processor builds interconnection with **system bus**. Typically, it contains *Control Bus*, *Data Bus*, and *Address Bus*. Inside the CPU, there are **Registers**, **ALU**, and **Control Unit** with control paths and a *Internal Processor Bus* to construct interconnection.


## Register Organization

{:.box-success}
At higher levels of the hierarchy, memory is faster, smaller, and more expensive - From *Memory Hierarchy*.

Registers are temporal storage in the processor. The level of register is higher than main memory and cache. There are mainly two register roles in the processor:

### User Visible Registers

{:.box-note}
Registers that enable **machine or programmer** to deploy in order to minimize the main memory references.

There are four main types of user-visible registers (however they are not clearly seperated):

- General Purpose (通用寄存器)
- Address
- Data
- Condition Codes, or *flags*

`True General-purpose regs` contain ***any operand for any opcode***. However in most cases these regs are dedicated to floating-point regs or stack regs. The number of GP regs range from 8 to 32. 

- Fewer number of GP regs increases memory references 
- More GP regs doesn't reduce memory referfences and take up more processor real estate(占用空间)

`Data regs` only hold data, while cannot be referenced in calculation of the address.

`Address regs` may be devoted to a specific address mode, which can be catagorized as:

- Stack pointer (For *Stack Addressing*)
- Index register (For *Autoindexing* or *Index addressing*)
- Segment Pointers (For system with segments)

`Condition Codes` are set of individual bits. They usually **cannot** be modified by programs.

Typical condition codes include:

- Sign
- Zero Flag
- Carry Flag
- Equal
- Overflow
- Interrupt Enable
- Supervisor: Allows **privileged instructions** to execute, not available to user programs
- etc.

They are more often appeared in PSWs.

Pros and Cons:

| Advantages | Disadvantages |
| :--------: | :-----------: |
| Reduced the number of `Compare` or `TEST` Commands | Add complexity to hardware and software |
| Conditional instructions are simplified | Require additional hdw connection |
| Easy to implement multiple branches | Need additional conditional-code-support instruction |
| Can be saved on stack | In pipelined implementation, it needs extra sync process |

{:.box-note}
*The register size is designed to hold a full address or word considering its type. Computers often permits **combining** two registers as one to use, such as type `long long` and `long double`.*

### Control and Status Registers 

{:.box-note}
Used by **Control Unit** to *control* the behavior of the processor, and by privileged operating system to *control* the execution of programs.

- PC: Program Counter, *point to the address of next instruction*.
- IR: Instruction Register, *contains instruction, not connected to any bus*
- MAR: Memory Address Register, *contains memory address, connected to address bus*
- MBR: Memory Buffer Register, *contains data, connected to data bus*

- Program Status Word (PSW): A or a set of regs containing status information *involving condition code*

## Instruction Cycle

A instruction cycle includes:

1. Fetch
2. Execute
3. Interrupt
4. Indirect

They are not sequentially organized, but interconnected.

<br/>
![Instruction Cycle](/assets/img/COA-images/Instructioncycle-simplified.png){: .mx-auto.d-block :}
<br/>

<center>Simplified Instruction Cycle Diagram</center>

{:.box-note}
In actual instruction cycle, there will be an **interruption check** during operand storage and next instruction fetch process in the diagram below.

<br/>
![Diagram](/assets/img/COA-images/instruction-cycle.png){: .mx-auto.d-block :}
<br/>

<center> Review Diagram </center>

### Data flow

In this part we will show the data flow of **Fetch**, **Indirect** and **Interrupt** process, as **execution** process requires no data flow in the processor.

{:.box-note}
Fetch Process Data flow are described below.<br/>
![Data Flow](/assets/img/COA-images/dataflow.png){: .mx-auto.d-block :}<br/><center> Fetch cycle data flow of a processor </center>

{:.box-note}
Indirect Process Data flow are described below.<br/>
![Data Flow](/assets/img/COA-images/dataflow-indirect.png){: .mx-auto.d-block :}<br/><center> Indirect cycle data flow of a processor </center>

{:.box-note}
Interrupt Process Data flow are described below.<br/>
![Data Flow](/assets/img/COA-images/indirect-interrupt.png){: .mx-auto.d-block :}<br/><center> Interrupt cycle data flow of a processor </center>


## Pipelining

### Six-stage pipeline for CPU

Here's a six-stage pipelined CPU operating diagram. Note that **unconditional branch** refers to operations like `JMP AX` which require no full instruction operated. **Branches and Interrupts** can contain conditions, like `JNZ AX`.
<br>
![Data Flow](/assets/img/COA-images/pipeline.png){: .mx-auto.d-block :}
<br/>

{:.box-note}
Every pipeline flush occurs **after** the `CO` or `EI` process has been done.

### Pipeline performance

Two main factors: **Time** and **Speed-up Factor**.

| Alphabet | Meaning | Formula |
| :------: | :------: | :------: |
| $$d$$ | time delay of a latch | / |
| $$\tau$$ | The time needed to advance a set of instructions **one stage** through the pipeline | $$\tau = \tau_{max} + d$$|
| $$T_{k,n}$$ | The **total time** required for a pipeline with *k* stages to execute *n* instructions | $$ T_{k,n} = (k + n - 1)\tau $$
| $$S_k$$ | Speedup factor for the instruction pipeline compared to execution without the pipeline | $$S_k = \frac{nk}{k + n - 1}$$ |

The diagram of speed-up factor with $$n$$ and $$k$$ shows that:

- More instructions are executed, more advantages are the pipelined structure shown. The curve will finally approach $$k$$.
- More pipeline stages are designed, the speed-up factor are more close to the instruction number fed into the non-pipelined structure (*i.e.*,$$n$$).
- However designing more pipeline stages encounters practical issues like *delay*, *cost*, etc. 

### Pipeline Hazard

{:.box-note}
Also referred to 'Pipeline bubble'


- Resource *(Structural)*: two (or more) instructions that are already in the pipeline **need the same resource**, the instruction(s) that are later fed in pipeline needs to be *stalled*. For example: `FI` and `FO`.

- Data: a conflict in the access of an operand location. Three types:
    - Read After Write, *i.e.* A read instruction followed by a write instruction, **hazard occurs when the latter instruction is executed earlier than the former.**
    - WAR
    - WAW

- Control *(Branch Hazard)*: the pipeline makes the wrong decision on a **branch prediction**, and brought wrong instructions into the pipeline.

### Dealing with Branches

Branches are the major setback of the pipelining design, as only when `EI` is done, the condition of a branch is determined. In real design, there are multiple ways to alleviate the problem:

- **Multiple stream**: Two pipelines with **the another prefetching the branch instructions**.
    - No washout
    - Bus and register **contention**(冲突)
    - Require multiple pipelines
- **Prefetch Branch Target**: Fetch branch target instruction and original one at the same time, until the branch is executed

- **Loop buffer**: Containing n **most recently fetched** instructions, very good for small loops. **The buffer is first looked up** in the fetch stage before the memory is referenced.
- **Branch prediction**
    - Static prediction: by predicting **never entering a branch**, **always ~** and by **opcode** (More accurate)
    - Dynamic prediction: Based on branch history. **If the last branch is taken, then the prediction will turn yes**, vice versa.
- **Delay branch**: Rearrange the instruction sequence so that the instruction with branch will be executed later.

# Problems 
